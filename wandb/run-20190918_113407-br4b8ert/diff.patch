diff --git a/RNN.ipynb b/RNN.ipynb
index bf14088..f7d13b6 100644
--- a/RNN.ipynb
+++ b/RNN.ipynb
@@ -9,7 +9,7 @@
      "data": {
       "text/html": [
        "\n",
-       "            Notebook configured with <a href=\"https://wandb.com\" target=\"_blank\">W&B</a>. You can <a href=\"https://app.wandb.ai/joaogui1/tensorflow2_learning/runs/p3oxikz0\" target=\"_blank\">open</a> the run page, or call <code>%%wandb</code>\n",
+       "            Notebook configured with <a href=\"https://wandb.com\" target=\"_blank\">W&B</a>. You can <a href=\"https://app.wandb.ai/joaogui1/tensorflow2_learning/runs/vhwxr9uf\" target=\"_blank\">open</a> the run page, or call <code>%%wandb</code>\n",
        "            in a cell containing your training loop to display live results.  Learn more in our <a href=\"https://docs.wandb.com/docs/integrations/jupyter.html\" target=\"_blank\">docs</a>.\n",
        "        "
       ],
@@ -96,57 +96,96 @@
      "text": [
       "Train on 2534 samples, validate on 1074 samples\n",
       "Epoch 1/1000\n",
-      "2534/2534 [==============================] - 23s 9ms/sample - loss: 9.9409 - mae: 9.9409 - val_loss: 10.5370 - val_mae: 10.5370\n",
+      "2534/2534 [==============================] - 13s 5ms/sample - loss: 9.9879 - mae: 9.9879 - val_loss: 10.5407 - val_mae: 10.5407\n",
       "Epoch 2/1000\n",
-      "2534/2534 [==============================] - 23s 9ms/sample - loss: 9.9282 - mae: 9.9282 - val_loss: 10.5357 - val_mae: 10.5357\n",
+      "2534/2534 [==============================] - 10s 4ms/sample - loss: 9.9283 - mae: 9.9283 - val_loss: 10.5361 - val_mae: 10.5361\n",
       "Epoch 3/1000\n",
-      "2534/2534 [==============================] - 24s 9ms/sample - loss: 9.9268 - mae: 9.9268 - val_loss: 10.5353 - val_mae: 10.5353\n",
+      "2534/2534 [==============================] - 10s 4ms/sample - loss: 9.9262 - mae: 9.9262 - val_loss: 10.5352 - val_mae: 10.5353\n",
       "Epoch 4/1000\n",
-      "2534/2534 [==============================] - 23s 9ms/sample - loss: 9.9262 - mae: 9.9261 - val_loss: 10.5351 - val_mae: 10.5351\n",
+      "2534/2534 [==============================] - 10s 4ms/sample - loss: 9.9258 - mae: 9.9258 - val_loss: 10.5350 - val_mae: 10.5351\n",
       "Epoch 5/1000\n",
-      "2534/2534 [==============================] - 23s 9ms/sample - loss: 9.9258 - mae: 9.9258 - val_loss: 10.5350 - val_mae: 10.5350\n",
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 9.9257 - mae: 9.9256 - val_loss: 10.5350 - val_mae: 10.5350\n",
       "Epoch 6/1000\n",
-      "2534/2534 [==============================] - 28s 11ms/sample - loss: 9.9257 - mae: 9.9257 - val_loss: 10.5350 - val_mae: 10.5350\n",
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 9.9256 - mae: 9.9255 - val_loss: 10.5350 - val_mae: 10.5350\n",
       "Epoch 7/1000\n",
-      "2534/2534 [==============================] - 29s 11ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5350 - val_mae: 10.5350\n",
+      "  14/2534 [..............................] - ETA: 9s - loss: 11.2358 - mae: 11.2358"
+     ]
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "/home/john/Documents/Programming/tensorflow2_learning/plotutil.py:61: RuntimeWarning: invalid value encountered in multiply\n",
+      "  plot.plot(np.append(np.empty_like(self.trainY) * np.nan, self.testY))\n",
+      "/home/john/Documents/Programming/tensorflow2_learning/plotutil.py:62: RuntimeWarning: invalid value encountered in multiply\n",
+      "  plot.plot(np.append(np.empty_like(self.trainY) * np.nan, preds))\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "2534/2534 [==============================] - 10s 4ms/sample - loss: 9.9257 - mae: 9.9257 - val_loss: 10.5350 - val_mae: 10.5349\n",
       "Epoch 8/1000\n",
-      "2534/2534 [==============================] - 33s 13ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5350 - val_mae: 10.5350\n",
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5350 - val_mae: 10.5349\n",
       "Epoch 9/1000\n",
-      "2534/2534 [==============================] - 28s 11ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5350 - val_mae: 10.5350\n",
+      "2534/2534 [==============================] - 10s 4ms/sample - loss: 9.9255 - mae: 9.9255 - val_loss: 10.5350 - val_mae: 10.5349\n",
       "Epoch 10/1000\n",
-      "2534/2534 [==============================] - 24s 9ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5350 - val_mae: 10.5350\n",
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5349 - val_mae: 10.5349\n",
       "Epoch 11/1000\n",
-      "2534/2534 [==============================] - 26s 10ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5350 - val_mae: 10.5350\n",
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 9.9255 - mae: 9.9255 - val_loss: 10.5350 - val_mae: 10.5349\n",
       "Epoch 12/1000\n",
-      "2534/2534 [==============================] - 28s 11ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5350 - val_mae: 10.5349\n",
+      "2534/2534 [==============================] - 10s 4ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5349 - val_mae: 10.5349\n",
       "Epoch 13/1000\n",
-      "2534/2534 [==============================] - 20s 8ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 9.9255 - mae: 9.9255 - val_loss: 10.5349 - val_mae: 10.5349\n",
       "Epoch 14/1000\n",
-      "2534/2534 [==============================] - 27s 11ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "2534/2534 [==============================] - 9s 3ms/sample - loss: 9.9254 - mae: 9.9254 - val_loss: 10.5350 - val_mae: 10.5349\n",
       "Epoch 15/1000\n",
-      "2534/2534 [==============================] - 33s 13ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 9.9257 - mae: 9.9256 - val_loss: 10.5349 - val_mae: 10.5349\n",
       "Epoch 16/1000\n",
-      "2534/2534 [==============================] - 29s 11ms/sample - loss: 9.9256 - mae: 9.9255 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "2534/2534 [==============================] - 10s 4ms/sample - loss: 9.9255 - mae: 9.9255 - val_loss: 10.5349 - val_mae: 10.5349\n",
       "Epoch 17/1000\n",
-      "2534/2534 [==============================] - 32s 13ms/sample - loss: 9.9256 - mae: 9.9255 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 9.9255 - mae: 9.9255 - val_loss: 10.5349 - val_mae: 10.5349\n",
       "Epoch 18/1000\n",
-      "2534/2534 [==============================] - 34s 13ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 9.9255 - mae: 9.9255 - val_loss: 10.5349 - val_mae: 10.5349\n",
       "Epoch 19/1000\n",
-      "2534/2534 [==============================] - 30s 12ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "2534/2534 [==============================] - 10s 4ms/sample - loss: 9.9255 - mae: 9.9255 - val_loss: 10.5349 - val_mae: 10.5349\n",
       "Epoch 20/1000\n",
-      "2534/2534 [==============================] - 25s 10ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "2534/2534 [==============================] - 10s 4ms/sample - loss: 9.9255 - mae: 9.9255 - val_loss: 10.5349 - val_mae: 10.5349\n",
       "Epoch 21/1000\n",
-      "2534/2534 [==============================] - 29s 11ms/sample - loss: 9.9256 - mae: 9.9255 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "2534/2534 [==============================] - 10s 4ms/sample - loss: 9.9255 - mae: 9.9255 - val_loss: 10.5349 - val_mae: 10.5349\n",
       "Epoch 22/1000\n",
-      "2534/2534 [==============================] - 32s 13ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 9.9254 - mae: 9.9254 - val_loss: 10.5349 - val_mae: 10.5349\n",
       "Epoch 23/1000\n",
-      "2534/2534 [==============================] - 32s 13ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 9.9255 - mae: 9.9255 - val_loss: 10.5349 - val_mae: 10.5349\n",
       "Epoch 24/1000\n",
-      "2534/2534 [==============================] - 34s 13ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "2534/2534 [==============================] - 10s 4ms/sample - loss: 9.9255 - mae: 9.9255 - val_loss: 10.5349 - val_mae: 10.5349\n",
       "Epoch 25/1000\n",
-      "2534/2534 [==============================] - 33s 13ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "2534/2534 [==============================] - 10s 4ms/sample - loss: 9.9255 - mae: 9.9255 - val_loss: 10.5349 - val_mae: 10.5349\n",
       "Epoch 26/1000\n",
-      " 157/2534 [>.............................] - ETA: 18s - loss: 10.1861 - mae: 10.1861"
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 9.9255 - mae: 9.9254 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "Epoch 27/1000\n",
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 9.9254 - mae: 9.9254 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "Epoch 28/1000\n",
+      "2534/2534 [==============================] - 10s 4ms/sample - loss: 9.9255 - mae: 9.9255 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "Epoch 29/1000\n",
+      "2534/2534 [==============================] - 10s 4ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "Epoch 30/1000\n",
+      "2534/2534 [==============================] - 10s 4ms/sample - loss: 9.9254 - mae: 9.9254 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "Epoch 31/1000\n",
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 9.9255 - mae: 9.9255 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "Epoch 32/1000\n",
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 9.9255 - mae: 9.9255 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "Epoch 33/1000\n",
+      "2534/2534 [==============================] - 9s 3ms/sample - loss: 9.9252 - mae: 9.9252 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "Epoch 34/1000\n",
+      "2534/2534 [==============================] - 9s 3ms/sample - loss: 9.9254 - mae: 9.9253 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "Epoch 35/1000\n",
+      "2534/2534 [==============================] - 9s 3ms/sample - loss: 9.9255 - mae: 9.9255 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "Epoch 36/1000\n",
+      "2534/2534 [==============================] - 9s 3ms/sample - loss: 9.9255 - mae: 9.9255 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "Epoch 37/1000\n",
+      "2138/2534 [========================>.....] - ETA: 1s - loss: 9.9215 - mae: 9.9215"
      ]
     }
    ],
@@ -158,13 +197,6 @@
     "model.compile(loss='mae', optimizer='adam', metrics=['mae'])\n",
     "model.fit(trainX, trainY, epochs=1000, batch_size=1, validation_data=(testX, testY),  callbacks=[WandbCallback(), PlotCallback(trainX, trainY, testX, testY, config.look_back, config.repeated_predictions)])"
    ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": []
   }
  ],
  "metadata": {
