diff --git a/RNN.ipynb b/RNN.ipynb
index bf14088..08570de 100644
--- a/RNN.ipynb
+++ b/RNN.ipynb
@@ -2,14 +2,14 @@
  "cells": [
   {
    "cell_type": "code",
-   "execution_count": 1,
+   "execution_count": 6,
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/html": [
        "\n",
-       "            Notebook configured with <a href=\"https://wandb.com\" target=\"_blank\">W&B</a>. You can <a href=\"https://app.wandb.ai/joaogui1/tensorflow2_learning/runs/p3oxikz0\" target=\"_blank\">open</a> the run page, or call <code>%%wandb</code>\n",
+       "            Notebook configured with <a href=\"https://wandb.com\" target=\"_blank\">W&B</a>. You can <a href=\"https://app.wandb.ai/joaogui1/tensorflow2_learning/runs/br4b8ert\" target=\"_blank\">open</a> the run page, or call <code>%%wandb</code>\n",
        "            in a cell containing your training loop to display live results.  Learn more in our <a href=\"https://docs.wandb.com/docs/integrations/jupyter.html\" target=\"_blank\">docs</a>.\n",
        "        "
       ],
@@ -42,7 +42,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 2,
+   "execution_count": 7,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -55,7 +55,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 3,
+   "execution_count": 8,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -70,10 +70,14 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 4,
+   "execution_count": 9,
    "metadata": {},
    "outputs": [],
    "source": [
+    "max_val = max(data)\n",
+    "min_val = min(data)\n",
+    "data=(data-min_val)/(max_val-min_val)\n",
+    "\n",
     "split = int(len(data) * 0.70)\n",
     "train = data[:split]\n",
     "test = data[split:]\n",
@@ -95,58 +99,141 @@
      "output_type": "stream",
      "text": [
       "Train on 2534 samples, validate on 1074 samples\n",
-      "Epoch 1/1000\n",
-      "2534/2534 [==============================] - 23s 9ms/sample - loss: 9.9409 - mae: 9.9409 - val_loss: 10.5370 - val_mae: 10.5370\n",
+      "Epoch 1/1000\n"
+     ]
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "WARNING: Logging before flag parsing goes to stderr.\n",
+      "W0918 08:34:12.591986 140033026828096 callbacks.py:243] Method (on_train_batch_end) is slow compared to the batch update (0.884153). Check your callbacks.\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "   1/2534 [..............................] - ETA: 1:10:06 - loss: 0.3975 - mae: 0.3975"
+     ]
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "W0918 08:34:12.602870 140033026828096 callbacks.py:243] Method (on_train_batch_end) is slow compared to the batch update (0.442091). Check your callbacks.\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "2534/2534 [==============================] - 11s 4ms/sample - loss: 0.0936 - mae: 0.0936 - val_loss: 0.0916 - val_mae: 0.0916\n",
       "Epoch 2/1000\n",
-      "2534/2534 [==============================] - 23s 9ms/sample - loss: 9.9282 - mae: 9.9282 - val_loss: 10.5357 - val_mae: 10.5357\n",
+      "  14/2534 [..............................] - ETA: 10s - loss: 0.1065 - mae: 0.1065"
+     ]
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "/home/john/Documents/Programming/tensorflow2_learning/plotutil.py:61: RuntimeWarning: invalid value encountered in multiply\n",
+      "  plot.plot(np.append(np.empty_like(self.trainY) * np.nan, self.testY))\n",
+      "/home/john/Documents/Programming/tensorflow2_learning/plotutil.py:62: RuntimeWarning: invalid value encountered in multiply\n",
+      "  plot.plot(np.append(np.empty_like(self.trainY) * np.nan, preds))\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 0.0822 - mae: 0.0822 - val_loss: 0.0767 - val_mae: 0.0767\n",
       "Epoch 3/1000\n",
-      "2534/2534 [==============================] - 24s 9ms/sample - loss: 9.9268 - mae: 9.9268 - val_loss: 10.5353 - val_mae: 10.5353\n",
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 0.0792 - mae: 0.0792 - val_loss: 0.0708 - val_mae: 0.0708\n",
       "Epoch 4/1000\n",
-      "2534/2534 [==============================] - 23s 9ms/sample - loss: 9.9262 - mae: 9.9261 - val_loss: 10.5351 - val_mae: 10.5351\n",
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 0.0766 - mae: 0.0766 - val_loss: 0.0709 - val_mae: 0.0709\n",
       "Epoch 5/1000\n",
-      "2534/2534 [==============================] - 23s 9ms/sample - loss: 9.9258 - mae: 9.9258 - val_loss: 10.5350 - val_mae: 10.5350\n",
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 0.0769 - mae: 0.0769 - val_loss: 0.0707 - val_mae: 0.0707\n",
       "Epoch 6/1000\n",
-      "2534/2534 [==============================] - 28s 11ms/sample - loss: 9.9257 - mae: 9.9257 - val_loss: 10.5350 - val_mae: 10.5350\n",
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 0.0757 - mae: 0.0757 - val_loss: 0.0689 - val_mae: 0.0689\n",
       "Epoch 7/1000\n",
-      "2534/2534 [==============================] - 29s 11ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5350 - val_mae: 10.5350\n",
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 0.0759 - mae: 0.0759 - val_loss: 0.0699 - val_mae: 0.0699\n",
       "Epoch 8/1000\n",
-      "2534/2534 [==============================] - 33s 13ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5350 - val_mae: 10.5350\n",
+      "2534/2534 [==============================] - 9s 3ms/sample - loss: 0.0754 - mae: 0.0754 - val_loss: 0.0702 - val_mae: 0.0702\n",
       "Epoch 9/1000\n",
-      "2534/2534 [==============================] - 28s 11ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5350 - val_mae: 10.5350\n",
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 0.0757 - mae: 0.0757 - val_loss: 0.0693 - val_mae: 0.0693\n",
       "Epoch 10/1000\n",
-      "2534/2534 [==============================] - 24s 9ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5350 - val_mae: 10.5350\n",
+      "2534/2534 [==============================] - 9s 3ms/sample - loss: 0.0756 - mae: 0.0756 - val_loss: 0.0744 - val_mae: 0.0744\n",
       "Epoch 11/1000\n",
-      "2534/2534 [==============================] - 26s 10ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5350 - val_mae: 10.5350\n",
+      "2534/2534 [==============================] - 9s 3ms/sample - loss: 0.0753 - mae: 0.0753 - val_loss: 0.0726 - val_mae: 0.0726\n",
       "Epoch 12/1000\n",
-      "2534/2534 [==============================] - 28s 11ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5350 - val_mae: 10.5349\n",
+      "2534/2534 [==============================] - 9s 3ms/sample - loss: 0.0754 - mae: 0.0754 - val_loss: 0.0724 - val_mae: 0.0724\n",
       "Epoch 13/1000\n",
-      "2534/2534 [==============================] - 20s 8ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "2534/2534 [==============================] - 9s 3ms/sample - loss: 0.0752 - mae: 0.0752 - val_loss: 0.0687 - val_mae: 0.0687\n",
       "Epoch 14/1000\n",
-      "2534/2534 [==============================] - 27s 11ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "2534/2534 [==============================] - 9s 3ms/sample - loss: 0.0751 - mae: 0.0751 - val_loss: 0.0690 - val_mae: 0.0690\n",
       "Epoch 15/1000\n",
-      "2534/2534 [==============================] - 33s 13ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "2534/2534 [==============================] - 9s 3ms/sample - loss: 0.0750 - mae: 0.0750 - val_loss: 0.0685 - val_mae: 0.0685\n",
       "Epoch 16/1000\n",
-      "2534/2534 [==============================] - 29s 11ms/sample - loss: 9.9256 - mae: 9.9255 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 0.0745 - mae: 0.0745 - val_loss: 0.0698 - val_mae: 0.0698\n",
       "Epoch 17/1000\n",
-      "2534/2534 [==============================] - 32s 13ms/sample - loss: 9.9256 - mae: 9.9255 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 0.0747 - mae: 0.0747 - val_loss: 0.0714 - val_mae: 0.0714\n",
       "Epoch 18/1000\n",
-      "2534/2534 [==============================] - 34s 13ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "2534/2534 [==============================] - 9s 3ms/sample - loss: 0.0746 - mae: 0.0746 - val_loss: 0.0700 - val_mae: 0.0700\n",
       "Epoch 19/1000\n",
-      "2534/2534 [==============================] - 30s 12ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "2534/2534 [==============================] - 9s 3ms/sample - loss: 0.0746 - mae: 0.0746 - val_loss: 0.0694 - val_mae: 0.0694\n",
       "Epoch 20/1000\n",
-      "2534/2534 [==============================] - 25s 10ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 0.0745 - mae: 0.0745 - val_loss: 0.0686 - val_mae: 0.0686\n",
       "Epoch 21/1000\n",
-      "2534/2534 [==============================] - 29s 11ms/sample - loss: 9.9256 - mae: 9.9255 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 0.0742 - mae: 0.0742 - val_loss: 0.0690 - val_mae: 0.0690\n",
       "Epoch 22/1000\n",
-      "2534/2534 [==============================] - 32s 13ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 0.0743 - mae: 0.0743 - val_loss: 0.0689 - val_mae: 0.0689\n",
       "Epoch 23/1000\n",
-      "2534/2534 [==============================] - 32s 13ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 0.0743 - mae: 0.0743 - val_loss: 0.0700 - val_mae: 0.0700\n",
       "Epoch 24/1000\n",
-      "2534/2534 [==============================] - 34s 13ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "2534/2534 [==============================] - 10s 4ms/sample - loss: 0.0746 - mae: 0.0746 - val_loss: 0.0683 - val_mae: 0.0683\n",
       "Epoch 25/1000\n",
-      "2534/2534 [==============================] - 33s 13ms/sample - loss: 9.9256 - mae: 9.9256 - val_loss: 10.5349 - val_mae: 10.5349\n",
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 0.0745 - mae: 0.0745 - val_loss: 0.0697 - val_mae: 0.0697\n",
       "Epoch 26/1000\n",
-      " 157/2534 [>.............................] - ETA: 18s - loss: 10.1861 - mae: 10.1861"
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 0.0743 - mae: 0.0743 - val_loss: 0.0689 - val_mae: 0.0689\n",
+      "Epoch 27/1000\n",
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 0.0743 - mae: 0.0743 - val_loss: 0.0721 - val_mae: 0.0721\n",
+      "Epoch 28/1000\n",
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 0.0746 - mae: 0.0746 - val_loss: 0.0688 - val_mae: 0.0688\n",
+      "Epoch 29/1000\n",
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 0.0743 - mae: 0.0743 - val_loss: 0.0690 - val_mae: 0.0690\n",
+      "Epoch 30/1000\n",
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 0.0745 - mae: 0.0745 - val_loss: 0.0686 - val_mae: 0.0686\n",
+      "Epoch 31/1000\n",
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 0.0744 - mae: 0.0744 - val_loss: 0.0696 - val_mae: 0.0696\n",
+      "Epoch 32/1000\n",
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 0.0742 - mae: 0.0742 - val_loss: 0.0688 - val_mae: 0.0688\n",
+      "Epoch 33/1000\n",
+      "2534/2534 [==============================] - 9s 4ms/sample - loss: 0.0746 - mae: 0.0746 - val_loss: 0.0687 - val_mae: 0.0687\n",
+      "Epoch 34/1000\n",
+      "2534/2534 [==============================] - 9s 3ms/sample - loss: 0.0743 - mae: 0.0743 - val_loss: 0.0711 - val_mae: 0.0711\n",
+      "Epoch 35/1000\n",
+      "2534/2534 [==============================] - 9s 3ms/sample - loss: 0.0745 - mae: 0.0745 - val_loss: 0.0682 - val_mae: 0.0682\n",
+      "Epoch 36/1000\n",
+      "2534/2534 [==============================] - 9s 3ms/sample - loss: 0.0742 - mae: 0.0742 - val_loss: 0.0685 - val_mae: 0.0685\n",
+      "Epoch 37/1000\n",
+      "2534/2534 [==============================] - 9s 3ms/sample - loss: 0.0744 - mae: 0.0744 - val_loss: 0.0698 - val_mae: 0.0698\n",
+      "Epoch 38/1000\n",
+      "2534/2534 [==============================] - 9s 3ms/sample - loss: 0.0742 - mae: 0.0742 - val_loss: 0.0687 - val_mae: 0.0687\n",
+      "Epoch 39/1000\n",
+      "2534/2534 [==============================] - 9s 3ms/sample - loss: 0.0745 - mae: 0.0745 - val_loss: 0.0699 - val_mae: 0.0699\n",
+      "Epoch 40/1000\n",
+      "2534/2534 [==============================] - 9s 3ms/sample - loss: 0.0745 - mae: 0.0745 - val_loss: 0.0687 - val_mae: 0.0687\n",
+      "Epoch 41/1000\n",
+      "2534/2534 [==============================] - 9s 3ms/sample - loss: 0.0743 - mae: 0.0743 - val_loss: 0.0686 - val_mae: 0.0686\n",
+      "Epoch 42/1000\n",
+      "2534/2534 [==============================] - 9s 3ms/sample - loss: 0.0745 - mae: 0.0745 - val_loss: 0.0685 - val_mae: 0.0685\n",
+      "Epoch 43/1000\n",
+      "2534/2534 [==============================] - 9s 3ms/sample - loss: 0.0744 - mae: 0.0744 - val_loss: 0.0688 - val_mae: 0.0688\n",
+      "Epoch 44/1000\n",
+      "2534/2534 [==============================] - 9s 3ms/sample - loss: 0.0744 - mae: 0.0744 - val_loss: 0.0688 - val_mae: 0.0688\n",
+      "Epoch 45/1000\n",
+      " 845/2534 [=========>....................] - ETA: 4s - loss: 0.0732 - mae: 0.0732"
      ]
     }
    ],
