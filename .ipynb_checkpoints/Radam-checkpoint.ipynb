{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.optimizer_v2.optimizer_v2 import OptimizerV2\n",
    "from tensorflow.python import ops, math_ops, state_ops, control_flow_ops\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "__all__ = ['RAdam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAdam(OptimizerV2):\n",
    "    \"\"\"RAdam optimizer.\n",
    "    According to the paper\n",
    "    [On The Variance Of The Adaptive Learning Rate And Beyond](https://arxiv.org/pdf/1908.03265v1.pdf).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 learning_rate=0.001,\n",
    "                 beta_1=0.9,\n",
    "                 beta_2=0.999,\n",
    "                 epsilon=1e-7,\n",
    "                 weight_decay=0.,\n",
    "                 amsgrad=False,\n",
    "                 total_steps=0,\n",
    "                 warmup_proportion=0.1,\n",
    "                 min_lr=0.,\n",
    "                 name='RAdam',\n",
    "                 **kwargs):\n",
    "        r\"\"\"Construct a new Adam optimizer.\n",
    "        Args:\n",
    "            learning_rate: A Tensor or a floating point value.    The learning rate.\n",
    "            beta_1: A float value or a constant float tensor. The exponential decay\n",
    "                rate for the 1st moment estimates.\n",
    "            beta_2: A float value or a constant float tensor. The exponential decay\n",
    "                rate for the 2nd moment estimates.\n",
    "            epsilon: A small constant for numerical stability. This epsilon is\n",
    "                \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n",
    "                Section 2.1), not the epsilon in Algorithm 1 of the paper.\n",
    "            weight_decay: A floating point value. Weight decay for each param.\n",
    "            amsgrad: boolean. Whether to apply AMSGrad variant of this algorithm from\n",
    "                the paper \"On the Convergence of Adam and beyond\".\n",
    "            total_steps: An integer. Total number of training steps.\n",
    "                Enable warmup by setting a positive value.\n",
    "            warmup_proportion: A floating point value. The proportion of increasing steps.\n",
    "            min_lr: A floating point value. Minimum learning rate after warmup.\n",
    "            name: Optional name for the operations created when applying gradients.\n",
    "                Defaults to \"Adam\".    @compatibility(eager) When eager execution is\n",
    "                enabled, `learning_rate`, `beta_1`, `beta_2`, and `epsilon` can each be\n",
    "                a callable that takes no arguments and returns the actual value to use.\n",
    "                This can be useful for changing these values across different\n",
    "                invocations of optimizer functions. @end_compatibility\n",
    "            **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`,\n",
    "                `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip\n",
    "                gradients by value, `decay` is included for backward compatibility to\n",
    "                allow time inverse decay of learning rate. `lr` is included for backward\n",
    "                compatibility, recommended to use `learning_rate` instead.\n",
    "        \"\"\"\n",
    "\n",
    "        super(RAdam, self).__init__(name, **kwargs)\n",
    "        self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))\n",
    "        self._set_hyper('beta_1', beta_1)\n",
    "        self._set_hyper('beta_2', beta_2)\n",
    "        self._set_hyper('decay', self._initial_decay)\n",
    "        self._set_hyper('weight_decay', weight_decay)\n",
    "        self._set_hyper('total_steps', float(total_steps))\n",
    "        self._set_hyper('warmup_proportion', warmup_proportion)\n",
    "        self._set_hyper('min_lr', min_lr)\n",
    "        self.epsilon = epsilon or K.epsilon()\n",
    "        self.amsgrad = amsgrad\n",
    "        self._initial_weight_decay = weight_decay\n",
    "        self._initial_total_steps = total_steps\n",
    "\n",
    "    def _create_slots(self, var_list):\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, 'm')\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, 'v')\n",
    "        if self.amsgrad:\n",
    "            for var in var_list:\n",
    "                self.add_slot(var, 'vhat')\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        params = self.weights\n",
    "        num_vars = int((len(params) - 1) / 2)\n",
    "        if len(weights) == 3 * num_vars + 1:\n",
    "            weights = weights[:len(params)]\n",
    "        super(RAdam, self).set_weights(weights)\n",
    "\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        var_dtype = var.dtype.base_dtype\n",
    "        lr_t = self._decayed_lr(var_dtype)\n",
    "        m = self.get_slot(var, 'm')\n",
    "        v = self.get_slot(var, 'v')\n",
    "        beta_1_t = self._get_hyper('beta_1', var_dtype)\n",
    "        beta_2_t = self._get_hyper('beta_2', var_dtype)\n",
    "        epsilon_t = ops.convert_to_tensor(self.epsilon, var_dtype)\n",
    "        local_step = math_ops.cast(self.iterations + 1, var_dtype)\n",
    "        beta_1_power = math_ops.pow(beta_1_t, local_step)\n",
    "        beta_2_power = math_ops.pow(beta_2_t, local_step)\n",
    "\n",
    "        if self._initial_total_steps > 0:\n",
    "            total_steps = self._get_hyper('total_steps', var_dtype)\n",
    "            warmup_steps = total_steps * self._get_hyper('warmup_proportion', var_dtype)\n",
    "            min_lr = self._get_hyper('min_lr', var_dtype)\n",
    "            decay_steps = K.maximum(total_steps - warmup_steps, 1)\n",
    "            decay_rate = (min_lr - lr_t) / decay_steps\n",
    "            lr_t = tf.where(\n",
    "                local_step <= warmup_steps,\n",
    "                lr_t * (local_step / warmup_steps),\n",
    "                lr_t + decay_rate * K.minimum(local_step - warmup_steps, decay_steps),\n",
    "            )\n",
    "\n",
    "        sma_inf = 2.0 / (1.0 - beta_2_t) - 1.0\n",
    "        sma_t = sma_inf - 2.0 * local_step * beta_2_power / (1.0 - beta_2_power)\n",
    "\n",
    "        m_t = state_ops.assign(m,\n",
    "                               beta_1_t * m + (1.0 - beta_1_t) * grad,\n",
    "                               use_locking=self._use_locking)\n",
    "        m_corr_t = m_t / (1.0 - beta_1_power)\n",
    "\n",
    "        v_t = state_ops.assign(v,\n",
    "                               beta_2_t * v + (1.0 - beta_2_t) * math_ops.square(grad),\n",
    "                               use_locking=self._use_locking)\n",
    "        if self.amsgrad:\n",
    "            vhat = self.get_slot(var, 'vhat')\n",
    "            vhat_t = state_ops.assign(vhat,\n",
    "                                      math_ops.maximum(vhat, v_t),\n",
    "                                      use_locking=self._use_locking)\n",
    "            v_corr_t = math_ops.sqrt(vhat_t / (1.0 - beta_2_power))\n",
    "        else:\n",
    "            vhat_t = None\n",
    "            v_corr_t = math_ops.sqrt(v_t / (1.0 - beta_2_power))\n",
    "\n",
    "        r_t = math_ops.sqrt((sma_t - 4.0) / (sma_inf - 4.0) *\n",
    "                            (sma_t - 2.0) / (sma_inf - 2.0) *\n",
    "                            sma_inf / sma_t)\n",
    "\n",
    "        var_t = tf.where(sma_t >= 5.0, r_t * m_corr_t / (v_corr_t + epsilon_t), m_corr_t)\n",
    "\n",
    "        if self._initial_weight_decay > 0.0:\n",
    "            var_t += self._get_hyper('weight_decay', var_dtype) * var\n",
    "\n",
    "        var_update = state_ops.assign_sub(var,\n",
    "                                          lr_t * var_t,\n",
    "                                          use_locking=self._use_locking)\n",
    "\n",
    "        updates = [var_update, m_t, v_t]\n",
    "        if self.amsgrad:\n",
    "            updates.append(vhat_t)\n",
    "        return control_flow_ops.group(*updates)\n",
    "\n",
    "    def _resource_apply_sparse(self, grad, var, indices):\n",
    "        var_dtype = var.dtype.base_dtype\n",
    "        lr_t = self._decayed_lr(var_dtype)\n",
    "        beta_1_t = self._get_hyper('beta_1', var_dtype)\n",
    "        beta_2_t = self._get_hyper('beta_2', var_dtype)\n",
    "        epsilon_t = ops.convert_to_tensor(self.epsilon, var_dtype)\n",
    "        local_step = math_ops.cast(self.iterations + 1, var_dtype)\n",
    "        beta_1_power = math_ops.pow(beta_1_t, local_step)\n",
    "        beta_2_power = math_ops.pow(beta_2_t, local_step)\n",
    "\n",
    "        if self._initial_total_steps > 0:\n",
    "            total_steps = self._get_hyper('total_steps', var_dtype)\n",
    "            warmup_steps = total_steps * self._get_hyper('warmup_proportion', var_dtype)\n",
    "            min_lr = self._get_hyper('min_lr', var_dtype)\n",
    "            decay_steps = K.maximum(total_steps - warmup_steps, 1)\n",
    "            decay_rate = (min_lr - lr_t) / decay_steps\n",
    "            lr_t = tf.where(\n",
    "                local_step <= warmup_steps,\n",
    "                lr_t * (local_step / warmup_steps),\n",
    "                lr_t + decay_rate * K.minimum(local_step - warmup_steps, decay_steps),\n",
    "            )\n",
    "\n",
    "        sma_inf = 2.0 / (1.0 - beta_2_t) - 1.0\n",
    "        sma_t = sma_inf - 2.0 * local_step * beta_2_power / (1.0 - beta_2_power)\n",
    "\n",
    "        m = self.get_slot(var, 'm')\n",
    "        m_scaled_g_values = grad * (1 - beta_1_t)\n",
    "        m_t = state_ops.assign(m, m * beta_1_t, use_locking=self._use_locking)\n",
    "        with ops.control_dependencies([m_t]):\n",
    "            m_t = self._resource_scatter_add(m, indices, m_scaled_g_values)\n",
    "        m_corr_t = m_t / (1.0 - beta_1_power)\n",
    "\n",
    "        v = self.get_slot(var, 'v')\n",
    "        v_scaled_g_values = (grad * grad) * (1 - beta_2_t)\n",
    "        v_t = state_ops.assign(v, v * beta_2_t, use_locking=self._use_locking)\n",
    "        with ops.control_dependencies([v_t]):\n",
    "            v_t = self._resource_scatter_add(v, indices, v_scaled_g_values)\n",
    "\n",
    "        if self.amsgrad:\n",
    "            vhat = self.get_slot(var, 'vhat')\n",
    "            vhat_t = state_ops.assign(vhat,\n",
    "                                      math_ops.maximum(vhat, v_t),\n",
    "                                      use_locking=self._use_locking)\n",
    "            v_corr_t = math_ops.sqrt(vhat_t / (1.0 - beta_2_power))\n",
    "        else:\n",
    "            vhat_t = None\n",
    "            v_corr_t = math_ops.sqrt(v_t / (1.0 - beta_2_power))\n",
    "\n",
    "        r_t = math_ops.sqrt((sma_t - 4.0) / (sma_inf - 4.0) *\n",
    "                            (sma_t - 2.0) / (sma_inf - 2.0) *\n",
    "                            sma_inf / sma_t)\n",
    "\n",
    "        var_t = tf.where(sma_t >= 5.0, r_t * m_corr_t / (v_corr_t + epsilon_t), m_corr_t)\n",
    "\n",
    "        if self._initial_weight_decay > 0.0:\n",
    "            var_t += self._get_hyper('weight_decay', var_dtype) * var\n",
    "\n",
    "        var_update = self._resource_scatter_add(var, indices, tf.gather(-lr_t * var_t, indices))\n",
    "\n",
    "        updates = [var_update, m_t, v_t]\n",
    "        if self.amsgrad:\n",
    "            updates.append(vhat_t)\n",
    "        return control_flow_ops.group(*updates)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(RAdam, self).get_config()\n",
    "        config.update({\n",
    "            'learning_rate': self._serialize_hyperparameter('learning_rate'),\n",
    "            'beta_1': self._serialize_hyperparameter('beta_1'),\n",
    "            'beta_2': self._serialize_hyperparameter('beta_2'),\n",
    "            'decay': self._serialize_hyperparameter('decay'),\n",
    "            'weight_decay': self._serialize_hyperparameter('weight_decay'),\n",
    "            'epsilon': self.epsilon,\n",
    "            'amsgrad': self.amsgrad,\n",
    "            'total_steps': self._serialize_hyperparameter('total_steps'),\n",
    "            'warmup_proportion': self._serialize_hyperparameter('warmup_proportion'),\n",
    "            'min_lr': self._serialize_hyperparameter('min_lr'),\n",
    "        })\n",
    "        return config"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
